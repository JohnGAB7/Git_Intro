{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e94db0-60b8-46a6-ab84-af564a0aa1c6",
   "metadata": {},
   "source": [
    "# Allianz Pricing: From Data Preparation to Risk Modeling (Data Preparation Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a94b1-0a0a-466d-84b7-7d854340fb39",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02ad57-faca-462a-bf25-a8d4bc0a795c",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Insert Context for Task 1]**\n",
    "\n",
    "In this task, we will discuss the need of data in the insurance context and how to practically prepare it for statistical analysis. Also, we will do some basic but practical common checks on claims and policy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efdb6df-8591-45e7-b3fe-227d11980e83",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "## Good to know\n",
    "\n",
    "This project assumes you have a solid foundation in Python, statistics, and machine learning. Before attempting this project, it is recommended that you complete the prerequisite courses.\n",
    "\n",
    "**[Insert Instructions Below]**\n",
    "- Before merging the policy data and claim data, check if the `IDpol` duplicates in both data sets. If yes, aggregate separate policies whose `IDpol` are same into one single policy.\n",
    "- Re-define the columns `IDpol` and `ClaimNb` as `int`. \n",
    "- Merge the policy and claim data (merging key: `IDpol`). For policies that don't have `ClaimAmount`, fill them with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96ae1e-796d-4864-b9c8-1c8441d23917",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2856bbb-b0f4-434d-834c-651daaf101e0",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy ID duplicates in policy data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# create data dictionary \n",
    "data_dic = pd.DataFrame.from_dict({\n",
    "    'IDpol': 'policy number (unique identifier)',\n",
    "    'ClaimNb': 'number of claims on the given policy',\n",
    "    'Exposure': 'total exposure in yearly units',\n",
    "    'Area': 'area code (categorical, ordinal)',\n",
    "    'VehPower': 'power of the car (categorical, ordinal)',\n",
    "    'VehAge': 'age of the car in years',\n",
    "    'DrivAge': 'age of the (most common) driver in years',\n",
    "    'BonusMalus': 'bonus-malus level between 50 and 230 (with base level 100)',\n",
    "    'VehBrand': 'car brand (categorical, nominal)',\n",
    "    'VehGas': 'diesel or regular fuel car (binary)',\n",
    "    'Density': 'density of inhabitants per km square in the city of the living place of the driver',\n",
    "    'Region': 'regions',\n",
    "    'ClaimAmount': 'claim amount on the given policy'}, \n",
    "    orient='index', columns=['definition'])\n",
    "\n",
    "# load both claim and policy data\n",
    "pol_db = pd.read_csv(\"datasets/policy_db.csv\")\n",
    "clm_db = pd.read_csv(\"datasets/claim_db.csv\")\n",
    "\n",
    "print(\"no duplicated policy ID in policy data\") if (len(pol_db) == len(set(pol_db))) else print(\"Policy ID duplicates in policy data\")\n",
    "\n",
    "# aggregate claim amount from same policy\n",
    "# aggregate separate policies whose `IDpol` are same into one single policy\n",
    "clm_db = clm_db.groupby(\"IDpol\").sum()\n",
    "\n",
    "# merge freq & sev datasets\n",
    "\n",
    "# Re-define the columns `IDpol` and `ClaimNb` as `int`.\n",
    "pol_db[\"IDpol\"]   = pol_db[\"IDpol\"].astype(int)\n",
    "pol_db[\"ClaimNb\"] = pol_db[\"ClaimNb\"].astype(int)\n",
    "pol_db.set_index(\"IDpol\", inplace=True)\n",
    "\n",
    "pol_clm_db = pol_db.join(clm_db, how=\"left\")\n",
    "pol_clm_db[\"ClaimAmount\"].fillna(0, inplace=True)\n",
    "pol_clm_db.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad6c605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy ID duplicates in policy data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pol_db = pd.read_csv(\"datasets/policy_db.csv\")\n",
    "clm_db = pd.read_csv(\"datasets/claim_db.csv\")\n",
    "\n",
    "print(\"no duplicated policy ID in policy data\") if (len(pol_db) == len(set(pol_db))) else print(\"Policy ID duplicates in policy data\")\n",
    "\n",
    "clm_db\n",
    "clm_db = clm_db.groupby(\"IDpol\").sum()\n",
    "clm_db\n",
    "pol_db[\"IDpol\"]   = pol_db[\"IDpol\"].astype(int)\n",
    "pol_db[\"ClaimNb\"] = pol_db[\"ClaimNb\"].astype(int)\n",
    "\n",
    "len(pol_db), len(clm_db)\n",
    "pol_db.set_index(\"IDpol\", inplace=True)\n",
    "\n",
    "#pol_clm_db = pol_db.join(clm_db, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1706aad-74c3-4ae8-b43a-7522c369b27c",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 2. Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601ad75-c08f-4627-a64d-856c31c9311f",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Insert Context for Task 2]**\n",
    "\n",
    "Get familiar with the new merged dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbffd8d-df71-47ea-853a-4907ff9861fc",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "- Use pandas dataframe.info() method to have a initial review on data information. \n",
    "- Check whether the number of rows remain the same after the data merge. \n",
    "- Check whether missing value exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea70f94-a97b-4b98-8b84-204354a984c8",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0144e537-728e-40b6-ad5a-88d46b4c1e25",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 678013 entries, 0 to 678012\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   IDpol        678013 non-null  float64\n",
      " 1   ClaimNb      678013 non-null  int32  \n",
      " 2   Exposure     678013 non-null  float64\n",
      " 3   Area         678013 non-null  object \n",
      " 4   VehPower     678013 non-null  float64\n",
      " 5   VehAge       678013 non-null  float64\n",
      " 6   DrivAge      678013 non-null  float64\n",
      " 7   BonusMalus   678013 non-null  float64\n",
      " 8   VehBrand     678013 non-null  object \n",
      " 9   VehGas       678013 non-null  object \n",
      " 10  Density      678013 non-null  float64\n",
      " 11  Region       678013 non-null  object \n",
      " 12  ClaimAmount  678013 non-null  float64\n",
      "dtypes: float64(8), int32(1), object(4)\n",
      "memory usage: 64.7+ MB\n"
     ]
    }
   ],
   "source": [
    "pol_clm_db.info()\n",
    "pol_clm_db.tail(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7bb15-c19c-4bef-ab4d-48b6361578a7",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 3. Preliminary data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6d4a1-ace5-4e0b-8584-15ac3de3d905",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Replace with Context for Task 3]**\n",
    "\n",
    "Get the basic KPI for the policy-claim data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41098ab2-e543-4a4d-ac21-516fa7a898af",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "- Group the data (`pol_clm_db`) with respect to the number of claims(`ClaimNb`) and calculate the total exposures (`Exposure`) and number of policies at each sub-group level of \"number of claims (`ClaimNb`)\".\n",
    "- Calculate the overall portfolio frequency, i.e. total number of claims divided by total exposure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587277e-bcc5-47ef-8b20-4d273cf3d2e7",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c2b107-b507-45b2-9b2f-119512a4d515",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "pol_clm_db.groupby(\"ClaimNb\")[[\"ClaimNb\", \"Exposure\"]].sum()\n",
    "pol_clm_db[\"Frequency\"] = pol_clm_db[\"ClaimNb\"] / pol_clm_db[\"Exposure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ed460-8f7d-4169-b866-fb3a53d4d922",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 4. Further analysis of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce95d0-8018-494f-a5da-43107c29da7c",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Insert Context for Task 4]**\n",
    "\n",
    "Determine whether policy renewal has happened during the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c74e4-d4b5-43c2-8f22-5c9cb7129ad6",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "\n",
    "- For policy whose exposure is equal to 1 year, what is the percentage of such policy. __Please note that the data used in this project only covers one whole calendar year.__ That means, if a policy has been renewed in reality but in our data set it's recorded as two separate policy. We can find their sum of exposure is 1. We can use this fact to detect whether policy renewal has happened.\n",
    "- Is the answer from (1) seems rather unusual? In practice, policies belonging to the same policy holder should share the same policy ID and is noted as new business or renewal. But we don't have such granular information in this dataset.\n",
    "- Determine whether the data set follow the practice mentioned above that same policy holder uses the same policy ID. If a policy had been renewed and was indeed recorded as two separate ones in this dataset, then the sum of their exposure should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbe2481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exposure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDpol</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.77000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.09000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>0.84000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114326.0</th>\n",
       "      <td>0.00274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114327.0</th>\n",
       "      <td>0.00274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114328.0</th>\n",
       "      <td>0.00274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114329.0</th>\n",
       "      <td>0.00274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114330.0</th>\n",
       "      <td>0.00274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>678013 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Exposure\n",
       "IDpol              \n",
       "1.0         0.10000\n",
       "3.0         0.77000\n",
       "5.0         0.75000\n",
       "10.0        0.09000\n",
       "11.0        0.84000\n",
       "...             ...\n",
       "6114326.0   0.00274\n",
       "6114327.0   0.00274\n",
       "6114328.0   0.00274\n",
       "6114329.0   0.00274\n",
       "6114330.0   0.00274\n",
       "\n",
       "[678013 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_clm_db.pivot_table(values = \"Exposure\", index = \"IDpol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a6ba4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDpol\n",
       "1.0          0.10000\n",
       "3.0          0.77000\n",
       "5.0          0.75000\n",
       "10.0         0.09000\n",
       "11.0         0.84000\n",
       "              ...   \n",
       "6114326.0    0.00274\n",
       "6114327.0    0.00274\n",
       "6114328.0    0.00274\n",
       "6114329.0    0.00274\n",
       "6114330.0    0.00274\n",
       "Name: Exposure, Length: 678013, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pol_clm_db.groupby(\"IDpol\")[\"Exposure\"].sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464a453-30af-4c7e-aa81-5cad288106cf",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "**[Insert Hints Below]**\n",
    "\n",
    "- groupby 'Area', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'VehBrand', 'VehGas', 'Density', 'Region', sum the exposure and count the number for each sub-group level. It's highly likely that same policy holder should share the same features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1140c5-0a63-4dca-a26d-0b1bf0d9fb60",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f6314-9d62-4425-8591-8989ed3a7fbf",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "_______________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad93fb-6bc3-47e3-b665-439adde75957",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 5. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43bb6d3-4d60-433d-94a2-41599a858d5d",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 5]**\n",
    "\n",
    "In this task, we will discuss how to modify the rating factors in order to make them usable for risk modeling. Also, we will introduce how to perform a univariate/multivariate analysis of each rating factor in terms of volumes and observed frequency/severity, which lay the ground for exploring dependence structure among feature components and for risk modeling as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e47b9f-8ca5-4ae4-8131-11e9ba47e5ab",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Add instructions]**\n",
    "\n",
    "- Plot empirical distribution for `Exposure` and `CliamNb`. Please also plot empirical distribution for those policies whose `Exposure` > 1 and `ClaimNb` > 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b399b-dc53-4ccb-9313-db9be685bf9d",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7ae9c-bf3d-4ee9-b0ad-f42c53932fc4",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15220810-4044-481a-93b7-eb53589b0dec",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 6. Explanatory variables: duplicates (bonus | difficult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b13b4-8a07-43db-aa83-a33df0db318b",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 6]**\n",
    "\n",
    "Comment based on the plots from last question: \n",
    "- A significant number of policies have low Exposure, i.e. close to zero.\n",
    "- Distribution of values is reasonable except that a spike (exposure = 0.08) occurs in `Exposure` plot (top left graph). We will further investigate it in the section below.\n",
    "\n",
    "\n",
    "Let's further look into two potential data errors: \n",
    "\n",
    "1. Policies belonging to the same policy holder should normally share same policy ID and be aggregated; Otherwise, when modelling, it will violate the assumption of independent sample.\n",
    "\n",
    "2. Duplicated policy: some policy is split into two parts where the sum of exposure equals the other policy(different `IDpol`) in this dataset.\n",
    "\n",
    "we don't fix the error above since it requires more granular information and dedicated efforts, which is not the main aim of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beabb736-d83b-43ab-af1f-50d93a4ccc84",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Add instructions]**\n",
    "\n",
    "Find any example/policy that might belong to the same policy holder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa383a-3c33-4543-8273-5af7d4087194",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e60dcf-ac3f-46f4-bd57-34c235e3b164",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ce032-66fe-494d-b249-69ca086774af",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 7. Abnormal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f794d3f-b725-4a17-a969-91eb216c7b75",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 7]**\n",
    "\n",
    "As mentioned previously, there is a spike in exposure histogram where exposure = 0.08. Figure out potential reasons for this abnormal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc6dd1-4592-4786-8b61-c972a5fcbedb",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Add Instructions]**\n",
    "\n",
    "- Figure out potential reasons for this abnormal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342651c3-406a-4f6b-894d-e93131edd657",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5dff7-c662-4b1f-b21b-cc32b3893166",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "pol_clm_db[pol_clm_db['Exposure']<1].hist(bins=100, column='Exposure')\n",
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b4e94-f7be-4434-9491-31bfc3294b13",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 8. Data anomaly: Large claim number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7c9c5-3e5c-4311-b70b-82481d57db19",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 8]**\n",
    "\n",
    "The largest claim number is 16, which is unusual for this data set. Try to find reason to explain such anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ea405-2f82-4655-869c-e5e0f1d0b308",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "- No instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebe89c-ce7d-4165-b289-0c5f11b0d982",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84693a0-eda9-4f59-90b7-1f15a16a8f47",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893cc31d-719a-4db5-9eaa-ec6bbb043a24",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 9. Dealing with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e19b5b-4de6-43ba-85ee-85a8f89e24e0",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 9]**\n",
    "\n",
    "Some policy contains certainly outliers (large claim count), and look very suspiciously like either poor data or fraudulent activity. Given the fields in the input data, we'll never be able to fully ascertain the true cause. The options are:\n",
    "\n",
    "1. Leave the data and hope any resulting model is unaffected. It may be a question of selecting a model specifically to be immune to this type of data.\n",
    "2. Remove these suspicious records from the data.\n",
    "3. Keep them but manually alter the data, e.g. cap the number of claims at 3 or 4, for example.\n",
    "\n",
    "For the time being, we will cap the number of claims at 4 in modeling part. In fact, it'll be interesting to fit and assess any model with and without these extreme observations, to see what difference it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee5072-36ff-4bd5-b555-09ac97e4b5a5",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "- Correct for those policy whose exposures are bigger than one year (by setting them equal to 1) since all observations are within one accounting year as per the data description. \n",
    "- It's easy to notice that 9 policies have more than 4 claims, the maximal number being 16. Please cap these values by 4 since these big values are likely to be data quote errors or fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da4a04-fc3d-4993-aacc-66c9c5d2caa5",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_______________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92497b70-db07-47b7-b903-f55c66fef852",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 10. One-way analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45def182-2fd2-449a-a3bf-3e7d7c1f1f11",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 10]**\n",
    "\n",
    "A one-way analysis summarizes insurance statistics, such as frequency or loss ratio, for each value of each explanatory variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f0a75-7bb5-4a92-87c1-8eeb54771c00",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "- Read the function below for the one-way analysis, use it for variable \"Area\" and see what you can find. Also you can play it with other variables and try to extract business insights from the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7bc45f-7607-473a-ba15-601e91822a08",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "def one_way_analysis(db, ow_var, expo_fr, expo_sev, response_fr, response_sev):\n",
    "    \n",
    "    _, axes = plt.subplots(nrows=2, ncols=2, figsize=(16,10))\n",
    "\n",
    "    response_fr    = db.groupby(ow_var)[response_fr].sum()\n",
    "    response_sev   = db.groupby(ow_var)[response_sev].sum()\n",
    "    expo_fr        = db.groupby(ow_var)[expo_fr].sum()\n",
    "    expo_sev       = db.groupby(ow_var)[expo_sev].sum()\n",
    "    ratio_fr       = response_fr / expo_fr\n",
    "    ratio_sev      = response_sev / expo_sev\n",
    "    \n",
    "    axes[0,0].title.set_text('Frequency')\n",
    "    axes[0,1].title.set_text('Severity')\n",
    "        \n",
    "    ratio_fr.plot(rot=0, ax=axes[0,0], style='o--')\n",
    "    expo_fr.plot.bar(rot=0, ax=axes[1,0])\n",
    "    \n",
    "    ratio_sev.plot(rot=0, ax=axes[0,1], style='o--')\n",
    "    expo_sev.plot.bar(rot=0, ax=axes[1,1])\n",
    "\n",
    "\n",
    "one_way_analysis(_______________)\n",
    "\n",
    "one_way_analysis(_______________)\n",
    "\n",
    "one_way_analysis(_______________)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ac272-cb7b-4f18-b4d6-4accdd51be7b",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 11. Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba451fe2-0580-4e6f-8aef-a187233ad2f1",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Add Context for Task 11]**\n",
    "\n",
    "Observation from one-way analysis:\n",
    "1. The marginal empirical frequency is monotonically increasing for the area code, suggesting that the feature component Area is ordinal.\n",
    "2. The marginal frequency is decreasing in vehicle age, and it has a \"U\" shape in driver's age (until age 50)\n",
    "3. We see that the frequency is monotonically increasing in the log density of the population\n",
    "4. By far the most explanatory feature component for frequency modeling is the bonus-malus level\n",
    "\n",
    "\n",
    "Based on the observation above, we can do some simplification for the explainable features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb8102-6f05-461b-a091-26505f9184cb",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Update Instructions]**\n",
    "\n",
    "- For variable `Area`, we found that, in frequency plot, the overall trend is upward. Please map the alphabet order into numerical order using dictionary `mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6}` \n",
    "- Capping variable `VehPower`, `VehAge`, `DrivAge` and `BonusMalus` at 9, 20, 90, 150, respectively, since the volume above these levels is negligible.\n",
    "- Log transformation for variable `Density`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874ecfb7-2d75-492f-88ed-8f78e92f0442",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda6a65-8318-4dac-9b0d-102acdee6385",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6} \n",
    "pol_clm_db['Area']          = _______________\n",
    "pol_clm_db['VehPower']      = _______________\n",
    "pol_clm_db['VehAge']        = _______________\n",
    "pol_clm_db['DrivAge']       = _______________\n",
    "pol_clm_db['BonusMalus']    = _______________\n",
    "pol_clm_db['LogDensity']    = round(np.log(___________), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652069e-7160-45b3-be7a-85d2b6f4c96c",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 12. Two-way analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9d2fe-269b-45b5-be30-f62fe455a6cb",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Add Context for Task 12]**\n",
    "\n",
    "We only do the numerical correlation in this case; It's also possible to get the correlation for categorical variables (e.g. Cramér's V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9deb58d-b547-4dce-8213-7507b15fdd8a",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Add Instructions for Task 12]**\n",
    "\n",
    "- Caculate correlation value for numberical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89720a2d-efc5-4b73-aba7-0c65f9674ceb",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91646a7e-7803-496e-88f5-20cb55e08378",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "pol_clm_db_corr = pol_clm_db.copy()\n",
    "\n",
    "vars = ['Area', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density'] #  'Area': catogory\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248de1e3-b473-4204-9636-3ad72d8c8766",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 13. Large loss analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56416322-980f-4956-9ec6-e98ef4bcb7fb",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 13]**\n",
    "\n",
    "A large loss is a claim event that typically occurs with a low frequency but large severity. Some negative impact on the risk modeling from large loss could be:\n",
    "- It's difficult to estimate large losses in the tail’s distribution between different years,\n",
    "- It will bias the estimation because some specific segments can be largely influenced by large losses. \n",
    "\n",
    "In this section, we will discuss how to deal with large cost in order to model a more stable quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c7463-939e-4d8a-a0c7-9187e883993f",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Add Instructions]**\n",
    "\n",
    "- Plot the empirical loss distribution for the whole data range and tail part. For tail threshold, one can choose certain value that he/she think it appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6693d33-9b72-4593-afa7-ded4ab69a809",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e92ac3-a6f0-4044-add7-3fd243c75384",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "sev_db = pol_clm_db.loc[pol_clm_db['ClaimAmount']>0,['ClaimNb','ClaimAmount']]\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,5))\n",
    "\n",
    "sev_db.hist(column='ClaimAmount', bins=100, log=True, ax=axes[0])\n",
    "sev_db[sev_db['ClaimAmount']>_____].hist(column='ClaimAmount', bins=100, ax=axes[1])\n",
    "\n",
    "axes[0].title.set_text('Claim Amount')\n",
    "axes[1].title.set_text('Tail behavior: Claim Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7ee7b-27ec-42a2-a3d9-1be80e19b9e5",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 14. Large loss analysis: Identifying thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98787030-619e-45a0-9254-bd304babcf2c",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Context for Task 14]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ad6b9-bd9e-40bd-ac02-dd6654477e85",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "- Identify threshold for large claim. Please note there are lots of ways to find the threshold, one can choose whatever method he/she thinks appropriate.\n",
    "- In this approach, we first get the percentage of claim amount over certain threshold, then we compute the percentage of claims number over the threshold. Finally, we define the metric, which is percentage of claim amount over threshold divided by percentage of claim number over threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e76136-d32a-48c3-9a1b-226c51de661d",
   "metadata": {
    "tags": [
     "@hint"
    ]
   },
   "source": [
    "**[Insert Hints Below]**\n",
    "\n",
    "- The approach here can be different as long as it can select a reasonable threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51157eab-2381-4080-92e9-a895654786e6",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc9862-ce0a-4c02-8d6f-d826e56ac6a5",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "________________________\n",
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93a914-9be8-460f-b4ea-f3106042cb48",
   "metadata": {},
   "source": [
    "### Solution Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37774bad-8447-4dde-9e4a-ce4f55bc007b",
   "metadata": {
    "tags": [
     "type:NotebookTask"
    ]
   },
   "source": [
    "## 15. Plotting distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af4fb7-e3dd-4791-a992-0c50b029e169",
   "metadata": {
    "tags": [
     "@context"
    ]
   },
   "source": [
    "**[Add Context for Task 15]**\n",
    "\n",
    "Visualize both attritional distribution and excess distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5eadbc-a935-47e8-8cf6-b8df4eb70ef9",
   "metadata": {
    "tags": [
     "@instructions"
    ]
   },
   "source": [
    "**[Instructions]**\n",
    "\n",
    "- Plot both attritional distribution and excess distribution given the large loss threshold equals 190,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a0e98-2667-48c6-965d-7c83b39028c0",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449b7a5-f82f-4287-b0da-d4324abcd581",
   "metadata": {
    "tags": [
     "@sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# attritional distribution\n",
    "threshold = 190_000\n",
    "_______________\n",
    "\n",
    "# excess distribution\n",
    "_______________"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb3127239cd96812d21f504294b44d4719e87dc6b1b58865b265735791272e38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
